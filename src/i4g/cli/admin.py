"""Administrative utilities for the i4g platform."""

from __future__ import annotations

import argparse
import json
import os
import subprocess
import sys
import uuid
import warnings
from collections.abc import Sequence as AbcSequence
from decimal import Decimal
from pathlib import Path
from typing import Any

from google.cloud import discoveryengine_v1beta as discoveryengine
from google.protobuf import json_format
from rich.console import Console

from i4g.reports.bundle_builder import BundleCriteria
from i4g.reports.dossier_pilot import (
    DEFAULT_PILOT_CASES_PATH,
    load_pilot_case_specs,
    schedule_pilot_plans,
    seed_pilot_cases,
)
from i4g.reports.dossier_queue_processor import DossierQueueProcessor
from i4g.services.factories import (
    build_bundle_builder,
    build_bundle_candidate_provider,
    build_review_store,
    build_vector_store,
)
from i4g.settings import get_settings
from i4g.task_status import TaskStatusReporter

# Fix for OpenMP runtime conflict on macOS.
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

console = Console()
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

SETTINGS = get_settings()


def _convert_struct(data: Any) -> Any:
    """Recursively convert proto-plus map/repeated containers into builtin types."""

    if isinstance(data, (str, int, float, bool)) or data is None:
        return data
    if hasattr(data, "items"):
        return {key: _convert_struct(value) for key, value in data.items()}
    if isinstance(data, AbcSequence) and not isinstance(data, (str, bytes, bytearray)):
        return [_convert_struct(value) for value in data]
    return data


def ensure_ollama_running() -> bool:
    """
    Quick connectivity check for Ollama.
    Returns True if the Ollama daemon is reachable, False otherwise.
    """
    try:
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True, timeout=3)
        return result.returncode == 0
    except Exception:
        return False


def run_query(args: argparse.Namespace) -> None:
    from i4g.rag.pipeline import build_scam_detection_chain

    if not ensure_ollama_running():
        console.print("[red]‚ùå Ollama is not running. Start it first with:[/red]")
        console.print("    ollama serve\n")
        sys.exit(1)

    console.print("[green]‚úÖ Ollama detected. Loading vectorstore...[/green]")

    try:
        store = build_vector_store(backend=args.backend)
    except Exception as e:  # pragma: no cover - defensive logging
        console.print(f"[red]Failed to initialize vectorstore:[/red] {e}")
        console.print("Make sure you ran `python scripts/build_index.py` successfully.")
        sys.exit(1)

    console.print("[cyan]üîó Building scam detection chain...[/cyan]")
    chain = build_scam_detection_chain(store)

    console.print(f"[cyan]ü§ñ Analyzing question:[/cyan] {args.question}\n")

    try:
        result = chain.invoke({"question": args.question})
    except Exception as e:  # pragma: no cover - defensive logging
        console.print(f"[red]‚ùå Query failed:[/red] {e}")
        sys.exit(1)

    console.print("\n[bold green]üß† Scam Detection Result:[/bold green]")
    console.print(result)

    console.print("\n[dim]Note: Results are generated by a local LLM using RAG context retrieval.[/dim]")


def run_vertex_search(args: argparse.Namespace) -> None:
    project = args.project or SETTINGS.vector.vertex_ai_project
    if not project:
        console.print("[red]‚ùå Provide --project or set I4G_VECTOR__VERTEX_AI__PROJECT.[/red]")
        sys.exit(1)

    location = args.location or SETTINGS.vector.vertex_ai_location or "global"
    data_store_id = args.data_store_id
    if not data_store_id:
        console.print("[red]‚ùå --data-store-id is required.[/red]")
        sys.exit(1)

    client = discoveryengine.SearchServiceClient()
    serving_config = client.serving_config_path(
        project=project,
        location=location,
        data_store=data_store_id,
        serving_config=args.serving_config_id,
    )

    request = discoveryengine.SearchRequest(
        serving_config=serving_config,
        query=args.query,
        page_size=args.page_size,
    )

    if args.filter_expression:
        request.filter = args.filter_expression

    if args.boost_json:
        try:
            boost_payload = json.loads(args.boost_json)
        except json.JSONDecodeError as exc:
            console.print(f"[red]‚ùå Failed to parse --boost-json:[/red] {exc}")
            sys.exit(1)

        boost_spec = discoveryengine.SearchRequest.BoostSpec()
        json_format.ParseDict(boost_payload, boost_spec._pb)
        request.boost_spec = boost_spec

    console.print(
        f"[cyan]üîç Vertex search:[/cyan] project={project} location={location} data_store={data_store_id} query='{args.query}'"
    )

    try:
        results = list(client.search(request=request))
    except Exception as exc:  # pragma: no cover - network failure
        console.print(f"[red]‚ùå Search failed:[/red] {exc}")
        sys.exit(1)

    if args.raw:
        payload = [json_format.MessageToDict(result._pb) for result in results]  # type: ignore[attr-defined]
        console.print_json(data=json.dumps(payload))
        return

    if not results:
        console.print("[yellow]No results returned.[/yellow]")
        return

    for rank, result in enumerate(results, start=1):
        document = result.document
        struct: dict[str, Any] = {}
        if document.json_data:
            try:
                struct = json.loads(document.json_data)
            except json.JSONDecodeError:
                struct = _convert_struct(document.struct_data) if document.struct_data else {}
        elif document.struct_data:
            struct = _convert_struct(document.struct_data)

        summary = struct.get("summary") or document.title or "<no summary>"
        label = struct.get("ground_truth_label") or "<unknown>"
        tags = ", ".join(struct.get("tags") or [])

        console.print(f"[bold cyan]#{rank}[/bold cyan] id={document.id} label={label}")
        if tags:
            console.print(f"    [dim]tags:[/dim] {tags}")
        console.print(f"    {summary}\n")


def export_saved_searches(args: argparse.Namespace) -> None:
    """Dump saved searches to JSON."""
    store = build_review_store()
    owner_filter = None if args.all else (args.owner or None)
    records = store.list_saved_searches(owner=owner_filter, limit=args.limit)
    include_tags = set(t.strip().lower() for t in (args.include_tags or []))
    if include_tags:
        records = [r for r in records if include_tags.intersection({t.lower() for t in (r.get("tags") or [])})]
    schema_version = args.schema_version.strip() if getattr(args, "schema_version", None) else ""
    for record in records:
        record.pop("created_at", None)
        if record.get("tags") is None:
            record["tags"] = []
        if schema_version:
            params = record.get("params") or {}
            if isinstance(params, dict):
                params["schema_version"] = schema_version
                record["params"] = params
    if args.split and args.output:
        base = Path(args.output)
        base.mkdir(parents=True, exist_ok=True)
        by_owner: dict[str, list[dict[str, object]]] = {}
        for record in records:
            owner = record.get("owner") or "shared"
            by_owner.setdefault(owner, []).append(record)
        for owner, rows in by_owner.items():
            fname = base / f"saved_searches_{owner}.json"
            fname.write_text(json.dumps(rows, indent=2))
            console.print(f"[green]‚úÖ Exported {len(rows)} saved search(es) to {fname}")
    else:
        data = json.dumps(records, indent=2)
        if args.output:
            Path(args.output).write_text(data)
            console.print(f"[green]‚úÖ Exported {len(records)} saved search(es) to {args.output}")
        else:
            console.print(data)


def import_saved_searches(args: argparse.Namespace) -> None:
    """Load saved searches from JSON file/stdin."""
    from i4g.api.review import SavedSearchImportRequest

    store = build_review_store()
    content = Path(args.input).read_text() if args.input else sys.stdin.read()
    try:
        payload = json.loads(content)
    except json.JSONDecodeError as exc:
        console.print(f"[red]‚ùå Invalid JSON:[/red] {exc}")
        sys.exit(1)

    include_tags = set(t.strip().lower() for t in (args.include_tags or []))
    items = payload if isinstance(payload, list) else [payload]
    imported = 0
    skipped = 0
    for item in items:
        try:
            req = SavedSearchImportRequest(**item)
            if include_tags and not include_tags.intersection({t.lower() for t in (req.tags or [])}):
                skipped += 1
                continue
            store.import_saved_search(req.model_dump(), owner=None if args.shared else args.owner)
            imported += 1
        except Exception as exc:  # pragma: no cover - defensive logging
            skipped += 1
            console.print(f"[yellow]Skipped #{imported + skipped}: {exc}[/yellow]")
    console.print(f"[green]‚úÖ Imported {imported} saved search(es); {skipped} skipped.")


def prune_saved_searches(args: argparse.Namespace) -> None:
    store = build_review_store()
    records = store.list_saved_searches(owner=args.owner, limit=1000)
    tags_filter = set(t.strip().lower() for t in (args.tags or []))
    to_delete = []
    for record in records:
        tags = {t.lower() for t in (record.get("tags") or [])}
        if tags_filter and not tags_filter.intersection(tags):
            continue
        to_delete.append(record)

    if not to_delete:
        console.print("[yellow]No saved searches matched the criteria.")
        return

    for record in to_delete:
        owner = record.get("owner") or "shared"
        console.print(f"[cyan]- {record.get('name')} (owner={owner}, tags={record.get('tags')})")

    if args.dry_run:
        console.print(f"[green]Dry run: {len(to_delete)} saved search(es) would be deleted.")
        return

    deleted = 0
    for record in to_delete:
        if store.delete_saved_search(record["search_id"]):
            deleted += 1
    console.print(f"[green]‚úÖ Deleted {deleted} saved search(es).")


def bulk_update_saved_search_tags(args: argparse.Namespace) -> None:
    if not any([args.add, args.remove, args.replace is not None]):
        console.print("[red]Provide --add, --remove, or --replace to adjust tags.[/red]")
        sys.exit(1)

    if args.replace is not None and (args.add or args.remove):
        console.print("[yellow]‚ö†Ô∏è --replace overrides --add/--remove; add/remove values will be ignored.[/yellow]")

    store = build_review_store()
    normalized_add = [t.strip() for t in (args.add or []) if t.strip()]
    normalized_remove = [t.strip() for t in (args.remove or []) if t.strip()]
    normalized_replace = [t.strip() for t in (args.replace or []) if t.strip()] if args.replace is not None else None

    summary_records = []
    target_ids = []

    if args.search_id:
        target_ids = [sid for sid in args.search_id if sid.strip()]
        for sid in target_ids:
            record = store.get_saved_search(sid)
            if record:
                summary_records.append(record)
    else:
        records = store.list_saved_searches(owner=args.owner, limit=args.limit)
        tags_filter = {t.strip().lower() for t in (args.tags or []) if t.strip()}
        if tags_filter:
            records = [r for r in records if tags_filter.intersection({t.lower() for t in (r.get("tags") or [])})]
        summary_records = records
        target_ids = [r["search_id"] for r in records]

    target_ids = list(dict.fromkeys(target_ids))

    if not target_ids:
        console.print("[yellow]No saved searches matched the criteria.")
        return

    if args.search_id:
        found_ids = {record["search_id"] for record in summary_records}
        missing_ids = [sid for sid in target_ids if sid not in found_ids]
        if missing_ids:
            console.print(
                "[yellow]Warning:[/yellow] the following saved search ID(s) were not found and will be skipped: "
                + ", ".join(missing_ids)
            )

    if args.dry_run:
        console.print(f"[green]Dry run:[/green] would update {len(target_ids)} saved search(es).")
        for record in summary_records[:10]:
            owner = record.get("owner") or "shared"
            console.print(f"  - {record.get('name')} (owner={owner}, tags={record.get('tags') or []})")
        if len(summary_records) > 10:
            console.print(f"  ...and {len(summary_records) - 10} more.")
        return

    updated = store.bulk_update_tags(
        target_ids,
        add=normalized_add,
        remove=normalized_remove,
        replace=normalized_replace,
    )
    console.print(f"[green]‚úÖ Updated tags for {updated} saved search(es).")


def build_dossiers(args: argparse.Namespace) -> None:
    provider = build_bundle_candidate_provider()
    candidates = provider.list_candidates(limit=args.limit)
    if not candidates:
        console.print(
            f"[yellow]No accepted cases found for bundling (limit={args.limit}). "
            "Review queue state before rerunning."
        )
        return

    min_loss_value = (
        Decimal(str(args.min_loss)) if args.min_loss is not None else Decimal(str(SETTINGS.report.min_loss_usd))
    )
    criteria = BundleCriteria(
        min_loss_usd=min_loss_value,
        recency_days=args.recency_days or SETTINGS.report.recency_days,
        max_cases_per_dossier=args.max_cases or SETTINGS.report.max_cases_per_dossier,
        jurisdiction_mode=args.jurisdiction_mode,
        require_cross_border=args.cross_border_only or SETTINGS.report.require_cross_border,
    )

    builder = build_bundle_builder()
    if args.dry_run:
        plans = builder.generate_plans(candidates=candidates, criteria=criteria)
        console.print(f"[cyan]‚ÑπÔ∏è Dry run:[/cyan] {len(plans)} dossier plan(s) would be created.")
        preview = min(args.preview, len(plans))
        for plan in plans[:preview]:
            console.print(
                "  - "
                f"{plan.plan_id} | cases={len(plan.cases)} | loss=${plan.total_loss_usd} | "
                f"jurisdiction={plan.jurisdiction_key} | cross_border={plan.cross_border}"
            )
        if len(plans) > preview:
            console.print(f"  ...and {len(plans) - preview} more plan(s).")
        return

    plan_ids = builder.build_and_enqueue(candidates=candidates, criteria=criteria)
    console.print(f"[green]‚úÖ Enqueued {len(plan_ids)} dossier plan(s) for agent processing.")


def process_dossiers(args: argparse.Namespace) -> None:
    processor = DossierQueueProcessor()
    task_id = args.task_id or os.getenv("I4G_TASK_ID")
    endpoint = args.task_status_url or os.getenv("I4G_TASK_STATUS_URL")
    if not task_id and endpoint:
        task_id = f"dossier-cli-{uuid.uuid4()}"

    reporter = TaskStatusReporter(task_id=task_id, endpoint=endpoint)
    if reporter.is_enabled():
        reporter.update(status="started", message="CLI dossier processing started", batch_size=args.batch_size)

    summary = processor.process_batch(
        batch_size=args.batch_size,
        dry_run=args.dry_run,
        reporter=reporter if reporter.is_enabled() else None,
    )
    if summary.processed == 0:
        console.print("[yellow]No pending dossier plans found in the queue.[/yellow]")
        return

    console.print(
        "[green]‚úÖ Processed {processed} plan(s) ‚Äî completed={completed} failed={failed} dry_run={dry}[/green]".format(
            processed=summary.processed,
            completed=summary.completed,
            failed=summary.failed,
            dry="yes" if summary.dry_run else "no",
        )
    )

    preview = min(args.preview, len(summary.plans))
    for plan in summary.plans[:preview]:
        status = plan.get("status")
        plan_id = plan.get("plan_id")
        artifacts = plan.get("artifacts") or []
        console.print(f"  - {plan_id} [{status}]")
        if artifacts:
            console.print(f"      artifacts: {artifacts}")
        if plan.get("warnings"):
            console.print(f"      warnings: {plan['warnings']}")
        if plan.get("error"):
            console.print(f"      error: {plan['error']}")


def schedule_pilot_dossiers(args: argparse.Namespace) -> None:
    cases_path = Path(args.cases_file).expanduser()
    try:
        specs = list(load_pilot_case_specs(cases_path))
    except Exception as exc:  # pragma: no cover - file IO errors surface here
        console.print(f"[red]‚ùå Failed to load pilot cases:[/red] {exc}")
        sys.exit(1)

    requested_ids = set()
    if args.cases:
        for raw in args.cases:
            for part in str(raw).split(","):
                value = part.strip()
                if value:
                    requested_ids.add(value)
    missing_from_config: list[str] = []
    if requested_ids:
        specs = [spec for spec in specs if spec.case_id in requested_ids]
        missing_from_config = sorted(requested_ids - {spec.case_id for spec in specs})

    if args.case_count:
        specs = specs[: args.case_count]

    if not specs:
        console.print("[red]‚ùå No pilot cases matched the provided filters.")
        sys.exit(1)

    seed_summary = seed_pilot_cases(specs)
    console.print(
        f"[green]‚úÖ Seeded {len(seed_summary.case_ids)} pilot case(s) into structured + review stores.[/green]"
    )

    if missing_from_config:
        console.print(
            "[yellow]‚ö†Ô∏è The following case_id(s) were not present in the pilot config:[/yellow] "
            + ", ".join(missing_from_config)
        )

    if args.seed_only:
        console.print("[cyan]‚ÑπÔ∏è Seed-only mode enabled; skipping dossier plan generation.")
        return

    min_loss = Decimal(str(args.min_loss)) if args.min_loss is not None else Decimal(str(SETTINGS.report.min_loss_usd))
    criteria = BundleCriteria(
        min_loss_usd=min_loss,
        recency_days=args.recency_days or SETTINGS.report.recency_days,
        max_cases_per_dossier=args.max_cases or SETTINGS.report.max_cases_per_dossier,
        jurisdiction_mode=args.jurisdiction_mode,
        require_cross_border=args.cross_border_only or SETTINGS.report.require_cross_border,
    )

    schedule_summary = schedule_pilot_plans(specs, criteria=criteria, dry_run=args.dry_run)

    if schedule_summary.missing_cases:
        console.print(
            "[yellow]‚ö†Ô∏è Candidate provider missing case_id(s):[/yellow] " + ", ".join(schedule_summary.missing_cases)
        )

    if not schedule_summary.plan_ids:
        console.print("[yellow]No dossier plans matched the pilot selection after filtering.")
        return

    if schedule_summary.dry_run:
        console.print(
            f"[cyan]‚ÑπÔ∏è Dry run: {len(schedule_summary.plan_ids)} plan(s) would be generated: "
            + ", ".join(schedule_summary.plan_ids)
        )
    else:
        console.print(
            f"[green]‚úÖ Enqueued {len(schedule_summary.plan_ids)} pilot plan(s): "
            + ", ".join(schedule_summary.plan_ids)
        )


def export_tag_presets(args: argparse.Namespace) -> None:
    store = build_review_store()
    presets = store.list_tag_presets(owner=args.owner, limit=1000)
    data = json.dumps(presets, indent=2)
    if args.output:
        Path(args.output).write_text(data)
        console.print(f"[green]‚úÖ Exported {len(presets)} tag preset(s) to {args.output}")
    else:
        console.print(data)


def import_tag_presets(args: argparse.Namespace) -> None:
    content = Path(args.input).read_text() if args.input else sys.stdin.read()
    try:
        payload = json.loads(content)
    except json.JSONDecodeError as exc:
        console.print(f"[red]‚ùå Invalid JSON:[/red] {exc}")
        sys.exit(1)
    items = payload if isinstance(payload, list) else [payload]
    presets = []
    for item in items:
        tags = item.get("tags") or []
        if tags and tags not in presets:
            presets.append(tags)
    if not presets:
        console.print("[yellow]No tag presets found in input.")
        return
    output = json.dumps(presets, indent=2)
    if args.input:
        Path(args.input).write_text(output)
        console.print(f"[green]‚úÖ Normalized {len(presets)} tag preset(s).")
    else:
        console.print(output)


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="i4g administrative utilities.")
    sub = parser.add_subparsers(dest="command", required=True)

    query = sub.add_parser("query", help="Run scam detection query.")
    query.add_argument("--question", required=True, help="User question text.")
    query.add_argument(
        "--backend",
        type=str,
        default=SETTINGS.vector.backend,
        choices=["chroma", "faiss"],
        help="Vector backend to use (overrides I4G_VECTOR_BACKEND).",
    )
    query.set_defaults(func=run_query)

    vertex = sub.add_parser("vertex-search", help="Query Vertex AI Search (Discovery) data store.")
    vertex.add_argument("query", help="Free-text query string to execute.")
    vertex.add_argument(
        "--project",
        default=SETTINGS.vector.vertex_ai_project,
        help="Google Cloud project containing the Discovery data store.",
    )
    vertex.add_argument(
        "--location",
        default=SETTINGS.vector.vertex_ai_location or "global",
        help="Discovery location (default: global).",
    )
    vertex.add_argument(
        "--data-store-id",
        required=True,
        help="Discovery data store identifier.",
    )
    vertex.add_argument(
        "--serving-config-id",
        default="default_search",
        help="Serving config identifier (default: default_search).",
    )
    vertex.add_argument(
        "--page-size",
        type=int,
        default=5,
        help="Maximum number of results to return (default: 5).",
    )
    vertex.add_argument(
        "--filter",
        dest="filter_expression",
        help="Optional filter expression (Discovery filter syntax).",
    )
    vertex.add_argument(
        "--boost-json",
        help="Optional BoostSpec payload expressed as JSON.",
    )
    vertex.add_argument(
        "--raw",
        action="store_true",
        help="Print raw JSON response instead of a formatted summary.",
    )
    vertex.set_defaults(func=run_vertex_search)

    export = sub.add_parser("export-saved-searches", help="Export saved searches to JSON.")
    export.add_argument("--limit", type=int, default=100, help="Max entries to export.")
    export.add_argument(
        "--all",
        action="store_true",
        help="Include shared searches along with personal ones.",
    )
    export.add_argument("--owner", help="Filter by owner username (ignored if --all).")
    export.add_argument("--output", help="Output file; omit for stdout.")
    export.add_argument(
        "--split",
        action="store_true",
        help="When writing to a folder, create one file per owner.",
    )
    export.add_argument(
        "--include-tags",
        nargs="*",
        help="Only export saved searches matching these tags (case-insensitive).",
    )
    export.add_argument(
        "--schema-version",
        default=SETTINGS.search.saved_search.schema_version,
        help="Optional schema version to inject into each exported search params.",
    )
    export.set_defaults(func=export_saved_searches)

    imp = sub.add_parser("import-saved-searches", help="Import saved searches from JSON.")
    imp.add_argument("--input", help="JSON file path (defaults to stdin).")
    imp.add_argument(
        "--owner",
        default=None,
        help="Owner username for imported searches (default: current user).",
    )
    imp.add_argument("--shared", action="store_true", help="Import into shared scope (owner=NULL).")
    imp.add_argument(
        "--include-tags",
        nargs="*",
        help="Only import saved searches that include these tags.",
    )
    imp.set_defaults(func=import_saved_searches)

    prune = sub.add_parser("prune-saved-searches", help="Delete saved searches by owner/tag filters.")
    prune.add_argument(
        "--owner",
        help="Delete saved searches belonging to this owner (omit for shared).",
    )
    prune.add_argument(
        "--tags",
        nargs="*",
        help="Only delete saved searches containing any of these tags.",
    )
    prune.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview deletions without applying them.",
    )
    prune.set_defaults(func=prune_saved_searches)

    bulk = sub.add_parser(
        "bulk-update-tags",
        help="Add, remove, or replace saved-search tags in bulk.",
    )
    bulk.add_argument("--owner", help="Filter saved searches to this owner (default: all owners).")
    bulk.add_argument(
        "--tags",
        nargs="*",
        help="Only target saved searches containing these tags (any match).",
    )
    bulk.add_argument(
        "--search-id",
        nargs="*",
        help="Explicit saved search IDs to update (skips owner/tag filters).",
    )
    bulk.add_argument("--add", nargs="+", help="Tags to add to each matched saved search.")
    bulk.add_argument("--remove", nargs="+", help="Tags to remove from each matched saved search.")
    bulk.add_argument(
        "--replace",
        nargs="+",
        help="Replace the existing tag set with this list (overrides --add/--remove).",
    )
    bulk.add_argument(
        "--limit",
        type=int,
        default=200,
        help="Max saved searches to inspect when filtering.",
    )
    bulk.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview the changes without persisting them.",
    )
    bulk.set_defaults(func=bulk_update_saved_search_tags)

    export_tags = sub.add_parser("export-tag-presets", help="Export tag presets derived from saved searches.")
    export_tags.add_argument("--owner", help="Filter presets to this owner (omit for shared).")
    export_tags.add_argument("--output", help="File to write JSON (stdout if omitted).")
    export_tags.set_defaults(func=export_tag_presets)

    import_tags = sub.add_parser("import-tag-presets", help="Import tag presets and append as filter presets.")
    import_tags.add_argument("--input", help="JSON file path (defaults to stdin).")
    import_tags.set_defaults(func=import_tag_presets)

    bundle = sub.add_parser(
        "build-dossiers",
        aliases=["bundle-dossiers"],
        help="Group accepted cases into dossier queue entries.",
    )
    bundle.add_argument("--limit", type=int, default=200, help="Number of accepted cases to inspect (default: 200).")
    bundle.add_argument(
        "--min-loss",
        type=float,
        default=None,
        help="Minimum loss threshold in USD (overrides settings.report.min_loss_usd).",
    )
    bundle.add_argument(
        "--recency-days",
        type=int,
        default=None,
        help="Accepted-within window in days (defaults to settings.report.recency_days).",
    )
    bundle.add_argument(
        "--max-cases",
        type=int,
        default=None,
        help="Maximum number of cases per dossier (defaults to settings.report.max_cases_per_dossier).",
    )
    bundle.add_argument(
        "--jurisdiction-mode",
        choices=["single", "multi", "global"],
        default="single",
        help="Grouping strategy for jurisdictions (default: single).",
    )
    bundle.add_argument(
        "--cross-border-only",
        action="store_true",
        help="Require cross-border cases regardless of settings.report.require_cross_border.",
    )
    bundle.add_argument("--dry-run", action="store_true", help="Show bundles without enqueuing them.")
    bundle.add_argument(
        "--preview",
        type=int,
        default=5,
        help="How many plans to display during --dry-run (default: 5).",
    )
    bundle.set_defaults(func=build_dossiers)

    process = sub.add_parser("process-dossiers", help="Lease queued dossier plans and render artifacts.")
    process.add_argument(
        "--batch-size",
        type=int,
        default=5,
        help="Number of queue entries to lease this run (default: 5).",
    )
    process.add_argument(
        "--preview",
        type=int,
        default=5,
        help="How many plan results to display after processing (default: 5).",
    )
    process.add_argument(
        "--dry-run",
        action="store_true",
        help="Inspect queue entries without generating artifacts.",
    )
    process.add_argument(
        "--task-id",
        help="Optional task identifier for Task_STATUS updates (overrides I4G_TASK_ID).",
    )
    process.add_argument(
        "--task-status-url",
        help="FastAPI /tasks base URL for status updates (overrides I4G_TASK_STATUS_URL).",
    )
    process.set_defaults(func=process_dossiers)

    pilot = sub.add_parser(
        "pilot-dossiers",
        help="Seed curated pilot cases and enqueue dossier plans for validation runs.",
    )
    pilot.add_argument(
        "--cases-file",
        default=str(DEFAULT_PILOT_CASES_PATH),
        help="Path to JSON file containing pilot case specs (default: data/manual_demo/dossier_pilot_cases.json).",
    )
    pilot.add_argument(
        "--case",
        dest="cases",
        action="append",
        help="Specific case_id(s) to include (repeat flag or provide comma-separated list).",
    )
    pilot.add_argument(
        "--case-count",
        type=int,
        help="Limit the number of pilot cases after filtering (default: all).",
    )
    pilot.add_argument(
        "--seed-only",
        action="store_true",
        help="Seed pilot data without generating or enqueuing dossier plans.",
    )
    pilot.add_argument(
        "--min-loss",
        type=float,
        default=None,
        help="Minimum loss threshold in USD (overrides settings.report.min_loss_usd).",
    )
    pilot.add_argument(
        "--recency-days",
        type=int,
        default=None,
        help="Accepted-within window in days (defaults to settings.report.recency_days).",
    )
    pilot.add_argument(
        "--max-cases",
        type=int,
        default=None,
        help="Maximum number of cases per dossier (defaults to settings.report.max_cases_per_dossier).",
    )
    pilot.add_argument(
        "--jurisdiction-mode",
        choices=["single", "multi", "global"],
        default="single",
        help="Grouping strategy for jurisdictions (default: single).",
    )
    pilot.add_argument(
        "--cross-border-only",
        action="store_true",
        help="Require cross-border cases regardless of settings.report.require_cross_border.",
    )
    pilot.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview plan IDs without enqueuing pilot dossiers.",
    )
    pilot.set_defaults(func=schedule_pilot_dossiers)

    return parser


def main(argv: list[str] | None = None) -> None:
    parser = build_parser()
    args = parser.parse_args(args=argv)
    args.func(args)


__all__ = [
    "build_parser",
    "main",
    "run_query",
    "run_vertex_search",
]
